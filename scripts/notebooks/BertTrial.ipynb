{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from data_preprocessing import Preprocess\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/dialects_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LY', 'MA', 'EG', 'LB', 'SD'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dialect'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess=Preprocess(df)\n",
    "\n",
    "cleaned_df = preprocess.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1009754958479151232</td>\n",
       "      <td>Ù‚Ù„ÙŠÙ„ÙŠÙ† Ø§Ø¯Ø¨ ÙˆÙ…Ù†Ø§ÙÙ‚ÙŠÙ† Ù„Ùˆ Ø§Ø®ØªÙ‡Ù… Ø§Ùˆ Ù‚Ø±ÙŠØ¨ØªÙ‡Ù… ØªØªØ¹Ø§ÙƒØ³...</td>\n",
       "      <td>LY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009794751548313600</td>\n",
       "      <td>Ø§Ù„Ù„ÙŠØ¨ÙŠÙŠÙ† Ù…ØªÙ‚Ù„Ø¨ÙŠÙ† Ø¨Ø³ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠØ§ Ø§Ù†Ø§ Ù…ÙŠÙ„ÙŠØ´ÙŠØ§ÙˆÙŠ ...</td>\n",
       "      <td>LY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1019989115490787200</td>\n",
       "      <td>ÙƒÙ„ ØªØ§Ù†ÙŠÙ‡ Ø´Ø§Ø¨ Ù„ÙŠØ¨ÙŠ Ø¨ÙŠØ±ØªØ§Ø­ Ù„Ø¨Ù†Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙŠÙ„Ø§Ø­Ø¸ Ø§Ù†...</td>\n",
       "      <td>LY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1035479791758135168</td>\n",
       "      <td>Ø±Ø§Ù†ÙŠØ§ Ø¹Ù‚Ù„ÙŠØªÙƒ Ù…ØªØ®Ù„ÙØ© Ø§ÙˆÙ„Ø§ Ø§Ù„Ø§Ù†Ø³Ø§Ù† ÙŠÙ„ÙŠ ÙŠØ­ØªØ§Ø¬ Ø§Ù‡Ù„...</td>\n",
       "      <td>LY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035481122921164800</td>\n",
       "      <td>Ø´ÙƒÙ„Ùƒ Ù…ØªØ¹Ù‚Ø¯Ø© Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø±Ø§Ø¬Ù„ Ù„ÙŠ ØªØ­Ø¨ÙŠÙ‡ Ø§Ø²ÙˆØ¬ Ø¨Ù†Øª ÙŠØªÙŠ...</td>\n",
       "      <td>LY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1009754958479151232  Ù‚Ù„ÙŠÙ„ÙŠÙ† Ø§Ø¯Ø¨ ÙˆÙ…Ù†Ø§ÙÙ‚ÙŠÙ† Ù„Ùˆ Ø§Ø®ØªÙ‡Ù… Ø§Ùˆ Ù‚Ø±ÙŠØ¨ØªÙ‡Ù… ØªØªØ¹Ø§ÙƒØ³...   \n",
       "1  1009794751548313600  Ø§Ù„Ù„ÙŠØ¨ÙŠÙŠÙ† Ù…ØªÙ‚Ù„Ø¨ÙŠÙ† Ø¨Ø³ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠØ§ Ø§Ù†Ø§ Ù…ÙŠÙ„ÙŠØ´ÙŠØ§ÙˆÙŠ ...   \n",
       "2  1019989115490787200  ÙƒÙ„ ØªØ§Ù†ÙŠÙ‡ Ø´Ø§Ø¨ Ù„ÙŠØ¨ÙŠ Ø¨ÙŠØ±ØªØ§Ø­ Ù„Ø¨Ù†Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙŠÙ„Ø§Ø­Ø¸ Ø§Ù†...   \n",
       "3  1035479791758135168  Ø±Ø§Ù†ÙŠØ§ Ø¹Ù‚Ù„ÙŠØªÙƒ Ù…ØªØ®Ù„ÙØ© Ø§ÙˆÙ„Ø§ Ø§Ù„Ø§Ù†Ø³Ø§Ù† ÙŠÙ„ÙŠ ÙŠØ­ØªØ§Ø¬ Ø§Ù‡Ù„...   \n",
       "4  1035481122921164800  Ø´ÙƒÙ„Ùƒ Ù…ØªØ¹Ù‚Ø¯Ø© Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø±Ø§Ø¬Ù„ Ù„ÙŠ ØªØ­Ø¨ÙŠÙ‡ Ø§Ø²ÙˆØ¬ Ø¨Ù†Øª ÙŠØªÙŠ...   \n",
       "\n",
       "  dialect  \n",
       "0      LY  \n",
       "1      LY  \n",
       "2      LY  \n",
       "3      LY  \n",
       "4      LY  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LY', 'MA', 'EG', 'LB', 'SD']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = cleaned_df['dialect'].unique().tolist()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'LY', 1: 'MA', 2: 'EG', 3: 'LB', 4: 'SD'},\n",
       " {'LY': 0, 'MA': 1, 'EG': 2, 'LB': 3, 'SD': 4})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = list(examples[\"text\"].values())\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "one=OneHotEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a column for each dialect\n",
    "one.fit(cleaned_df[['dialect']])\n",
    "dialects=one.transform(cleaned_df[['dialect']]).toarray()\n",
    "dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "      <th>LY</th>\n",
       "      <th>MA</th>\n",
       "      <th>EG</th>\n",
       "      <th>LB</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1009754958479151232</td>\n",
       "      <td>Ù‚Ù„ÙŠÙ„ÙŠÙ† Ø§Ø¯Ø¨ ÙˆÙ…Ù†Ø§ÙÙ‚ÙŠÙ† Ù„Ùˆ Ø§Ø®ØªÙ‡Ù… Ø§Ùˆ Ù‚Ø±ÙŠØ¨ØªÙ‡Ù… ØªØªØ¹Ø§ÙƒØ³...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009794751548313600</td>\n",
       "      <td>Ø§Ù„Ù„ÙŠØ¨ÙŠÙŠÙ† Ù…ØªÙ‚Ù„Ø¨ÙŠÙ† Ø¨Ø³ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠØ§ Ø§Ù†Ø§ Ù…ÙŠÙ„ÙŠØ´ÙŠØ§ÙˆÙŠ ...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1019989115490787200</td>\n",
       "      <td>ÙƒÙ„ ØªØ§Ù†ÙŠÙ‡ Ø´Ø§Ø¨ Ù„ÙŠØ¨ÙŠ Ø¨ÙŠØ±ØªØ§Ø­ Ù„Ø¨Ù†Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙŠÙ„Ø§Ø­Ø¸ Ø§Ù†...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1035479791758135168</td>\n",
       "      <td>Ø±Ø§Ù†ÙŠØ§ Ø¹Ù‚Ù„ÙŠØªÙƒ Ù…ØªØ®Ù„ÙØ© Ø§ÙˆÙ„Ø§ Ø§Ù„Ø§Ù†Ø³Ø§Ù† ÙŠÙ„ÙŠ ÙŠØ­ØªØ§Ø¬ Ø§Ù‡Ù„...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035481122921164800</td>\n",
       "      <td>Ø´ÙƒÙ„Ùƒ Ù…ØªØ¹Ù‚Ø¯Ø© Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø±Ø§Ø¬Ù„ Ù„ÙŠ ØªØ­Ø¨ÙŠÙ‡ Ø§Ø²ÙˆØ¬ Ø¨Ù†Øª ÙŠØªÙŠ...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1009754958479151232  Ù‚Ù„ÙŠÙ„ÙŠÙ† Ø§Ø¯Ø¨ ÙˆÙ…Ù†Ø§ÙÙ‚ÙŠÙ† Ù„Ùˆ Ø§Ø®ØªÙ‡Ù… Ø§Ùˆ Ù‚Ø±ÙŠØ¨ØªÙ‡Ù… ØªØªØ¹Ø§ÙƒØ³...   \n",
       "1  1009794751548313600  Ø§Ù„Ù„ÙŠØ¨ÙŠÙŠÙ† Ù…ØªÙ‚Ù„Ø¨ÙŠÙ† Ø¨Ø³ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠØ§ Ø§Ù†Ø§ Ù…ÙŠÙ„ÙŠØ´ÙŠØ§ÙˆÙŠ ...   \n",
       "2  1019989115490787200  ÙƒÙ„ ØªØ§Ù†ÙŠÙ‡ Ø´Ø§Ø¨ Ù„ÙŠØ¨ÙŠ Ø¨ÙŠØ±ØªØ§Ø­ Ù„Ø¨Ù†Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙŠÙ„Ø§Ø­Ø¸ Ø§Ù†...   \n",
       "3  1035479791758135168  Ø±Ø§Ù†ÙŠØ§ Ø¹Ù‚Ù„ÙŠØªÙƒ Ù…ØªØ®Ù„ÙØ© Ø§ÙˆÙ„Ø§ Ø§Ù„Ø§Ù†Ø³Ø§Ù† ÙŠÙ„ÙŠ ÙŠØ­ØªØ§Ø¬ Ø§Ù‡Ù„...   \n",
       "4  1035481122921164800  Ø´ÙƒÙ„Ùƒ Ù…ØªØ¹Ù‚Ø¯Ø© Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø±Ø§Ø¬Ù„ Ù„ÙŠ ØªØ­Ø¨ÙŠÙ‡ Ø§Ø²ÙˆØ¬ Ø¨Ù†Øª ÙŠØªÙŠ...   \n",
       "\n",
       "  dialect   LY   MA   EG   LB   SD  \n",
       "0      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "1      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "2      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "3      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "4      LY  0.0  0.0  1.0  0.0  0.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df[labels] = dialects\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "      <th>LY</th>\n",
       "      <th>MA</th>\n",
       "      <th>EG</th>\n",
       "      <th>LB</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1009754958479151232</td>\n",
       "      <td>Ù‚Ù„ÙŠÙ„ÙŠÙ† Ø§Ø¯Ø¨ ÙˆÙ…Ù†Ø§ÙÙ‚ÙŠÙ† Ù„Ùˆ Ø§Ø®ØªÙ‡Ù… Ø§Ùˆ Ù‚Ø±ÙŠØ¨ØªÙ‡Ù… ØªØªØ¹Ø§ÙƒØ³...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009794751548313600</td>\n",
       "      <td>Ø§Ù„Ù„ÙŠØ¨ÙŠÙŠÙ† Ù…ØªÙ‚Ù„Ø¨ÙŠÙ† Ø¨Ø³ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠØ§ Ø§Ù†Ø§ Ù…ÙŠÙ„ÙŠØ´ÙŠØ§ÙˆÙŠ ...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1019989115490787200</td>\n",
       "      <td>ÙƒÙ„ ØªØ§Ù†ÙŠÙ‡ Ø´Ø§Ø¨ Ù„ÙŠØ¨ÙŠ Ø¨ÙŠØ±ØªØ§Ø­ Ù„Ø¨Ù†Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙŠÙ„Ø§Ø­Ø¸ Ø§Ù†...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1035479791758135168</td>\n",
       "      <td>Ø±Ø§Ù†ÙŠØ§ Ø¹Ù‚Ù„ÙŠØªÙƒ Ù…ØªØ®Ù„ÙØ© Ø§ÙˆÙ„Ø§ Ø§Ù„Ø§Ù†Ø³Ø§Ù† ÙŠÙ„ÙŠ ÙŠØ­ØªØ§Ø¬ Ø§Ù‡Ù„...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1035481122921164800</td>\n",
       "      <td>Ø´ÙƒÙ„Ùƒ Ù…ØªØ¹Ù‚Ø¯Ø© Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø±Ø§Ø¬Ù„ Ù„ÙŠ ØªØ­Ø¨ÙŠÙ‡ Ø§Ø²ÙˆØ¬ Ø¨Ù†Øª ÙŠØªÙŠ...</td>\n",
       "      <td>LY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1009754958479151232  Ù‚Ù„ÙŠÙ„ÙŠÙ† Ø§Ø¯Ø¨ ÙˆÙ…Ù†Ø§ÙÙ‚ÙŠÙ† Ù„Ùˆ Ø§Ø®ØªÙ‡Ù… Ø§Ùˆ Ù‚Ø±ÙŠØ¨ØªÙ‡Ù… ØªØªØ¹Ø§ÙƒØ³...   \n",
       "1  1009794751548313600  Ø§Ù„Ù„ÙŠØ¨ÙŠÙŠÙ† Ù…ØªÙ‚Ù„Ø¨ÙŠÙ† Ø¨Ø³ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙŠØ§ Ø§Ù†Ø§ Ù…ÙŠÙ„ÙŠØ´ÙŠØ§ÙˆÙŠ ...   \n",
       "2  1019989115490787200  ÙƒÙ„ ØªØ§Ù†ÙŠÙ‡ Ø´Ø§Ø¨ Ù„ÙŠØ¨ÙŠ Ø¨ÙŠØ±ØªØ§Ø­ Ù„Ø¨Ù†Øª Ù…Ø®ØªÙ„ÙØ© ÙˆÙŠÙ„Ø§Ø­Ø¸ Ø§Ù†...   \n",
       "3  1035479791758135168  Ø±Ø§Ù†ÙŠØ§ Ø¹Ù‚Ù„ÙŠØªÙƒ Ù…ØªØ®Ù„ÙØ© Ø§ÙˆÙ„Ø§ Ø§Ù„Ø§Ù†Ø³Ø§Ù† ÙŠÙ„ÙŠ ÙŠØ­ØªØ§Ø¬ Ø§Ù‡Ù„...   \n",
       "4  1035481122921164800  Ø´ÙƒÙ„Ùƒ Ù…ØªØ¹Ù‚Ø¯Ø© Ø¹Ù„Ø´Ø§Ù† Ø§Ù„Ø±Ø§Ø¬Ù„ Ù„ÙŠ ØªØ­Ø¨ÙŠÙ‡ Ø§Ø²ÙˆØ¬ Ø¨Ù†Øª ÙŠØªÙŠ...   \n",
       "\n",
       "  dialect   LY   MA   EG   LB   SD  \n",
       "0      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "1      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "2      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "3      LY  0.0  0.0  1.0  0.0  0.0  \n",
       "4      LY  0.0  0.0  1.0  0.0  0.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = train_test_split(cleaned_df.drop(\"id\",axis=1), test_size=0.2, random_state=42,stratify=cleaned_df['dialect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(x_train)\n",
    "test_dataset = Dataset.from_pandas(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'ÙˆØ§Ù„Ù„Ù‡ Ø¨Ø§Ù„Ø¹ÙƒØ³ Ø¬Ùˆ Ø³Ù…Ø­ Ù„ÙƒÙ† ØµÙ‚Ø¹ Ø³Ù… Ù„ÙŠÙ† Ø®Ù„Ø§Øµ',\n",
       " 'dialect': 'LY',\n",
       " 'LY': 0.0,\n",
       " 'MA': 0.0,\n",
       " 'EG': 1.0,\n",
       " 'LB': 0.0,\n",
       " 'SD': 0.0,\n",
       " '__index_level_0__': 30200}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"text\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6949efe220a4ef9b8db9eb94827bc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/118180 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3805c160f64749bd883b36fd2e1a3758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset_dict.map(preprocess_data, batched=True, remove_columns=dataset_dict['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ÙˆØ§Ù„Ù„Ù‡ Ø¨Ø§Ù„Ø¹ÙƒØ³ Ø¬Ùˆ Ø³Ù…Ø­ Ù„ÙƒÙ† ØµÙ‚Ø¹ Ø³Ù… Ù„ÙŠÙ† Ø®Ù„Ø§Øµ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EG']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "Here we define a model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top. One should fine-tune this head, together with the pre-trained base on a labeled dataset.\n",
    "\n",
    "This is also printed by the warning.\n",
    "\n",
    "We set the `problem_type` to be \"multi_label_classification\", as this will make sure the appropriate loss function is used (namely [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1298, 25573, 23673, 23673, 14157,  1271, 25573, 23673, 29830,\n",
       "        29835, 29824,  1275, 29836,  1282, 22192, 29820,  1294, 29835, 15915,\n",
       "         1284, 29834, 29830,  1282, 22192,  1294, 14498, 15915,  1277, 23673,\n",
       "        25573, 29826,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6631, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.1042, -0.4823,  0.0990,  0.2309,  0.0768]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"f1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move the model to the GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e6925a695b488f93185d1bf36b0e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1886\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1887\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1888\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1890\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\transformers\\trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\transformers\\trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\accelerate\\accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2013\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zacks\\anaconda3\\envs\\DataTraining\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataTraining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
